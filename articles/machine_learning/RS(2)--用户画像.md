> 总第 122 篇文章，本文大约 5100 字，阅读大约需要  15 分钟

上一篇文章简单介绍了[推荐系统的定义和应用](https://mp.weixin.qq.com/s/JUqYRtzlefWw-GZN9eTCqQ)，推荐系统第二篇，简单介绍用户画像的知识， 以及通过文本来构建用户画像的知识。

目录如下：

- 用户画像
  - 用户画像的定义
  - 用户画像的关键
  - 构建用户画像的方法
- 从文本到用户啊画像
  - 构建用户画像
  - 结构化文本
  - 标签选择
- 小结



------

### 用户画像

#### 用户画像的定义

用户画像其实就是从海量的用户数据中，**建模抽象出来每个用户的属性标签体系**，这些属性通常需要具有一定的商业价值。

而如果从计算机的角度，**用户画像是对用户信息的向量化表示**，向量化是为了给计算机计算，用户画像应该是给机器看的，而不是人看的。

用户标签体系一般分为多个大类（一级分类），每个大类下有多个小分类（二级分类），小分类下面还可以继续再划分更小的三级、四级等分类，大分类通常包括这几种：

- **人口属性**。用户固有属性，比如年龄性别等；
- **兴趣偏好**。用户的个人偏好，包括品类便好、品牌便好、距离便好、业务单元便好等；
- **特征人群**。具有特定意义的人群划分，比如学生、旅游达人、有车一族、母婴、吃货等；
- **用户分级**。区分用户的层级划分，比如会员等级、消费水平、优惠敏感度等；
- **LBS属性**。和用户位置相关的各种属性，比如用户的常驻城市和国家、家乡、用户足迹、居住商圈、工作商圈等

对于一个推荐系统，用户画像并不是它的目的，而是在构建推荐系统的过程中**产生的一个关键环节的副产品**。

通常推荐系统会分为召回和排序两个阶段，在这两个阶段中都可能会用到用户画像。



#### 用户画像的关键

用户画像的关键元素是**维度和量化**。

##### 维度

**维度首先要保证每个维度的名称是可理解的**，比如推荐一部手机给用户的时候，维度包括价格、手机品牌、手机的内存大小、外观等，这里需要保证用户和手机的维度能够匹配上；

**维度的数量一般越多**，用户画像也越精细，但计算代价也会变大，需要做一个权衡。

具体采用哪些维度，需要以推荐的目的为主，而不是为了用户画像而做用户画像，比如目的是提高点击率，那就需要考虑哪些因素可能会影响用户点击产品，比如推荐手机的时候，手机的品牌、价格、配置因素，都会影响用户是否会点击查看手机的具体信息，因此手机产品的标题都会标明这些很重要的信息，比如：“iphone 11 128g， 5999元”。

##### 量化

对用户画像的量化，其实就是对数据的处理方式，也可以说就是特征工程，应该以目标为导向，根据推荐效果为查看具体采用哪种量化的方法。



#### 构建用户画像的方法

按照对用户量化的手段来分，可以分成3类方法

##### 1. 查户口

**直接采用原始数据作为用户画像的内容**，比如注册资料等人口统计学信息，或者是购买、浏览历史，这种通常只是做了数据清洗的工作，数据本身没有做任何抽象和归纳，**通常对用户冷启动等场景非常有用**。



##### 2. 堆数据

方法就是堆积历史数据，做统计工作，也是最常见的用户画像数据，比如常见的兴趣标签，从历史行为中去挖掘出这些标签，然后在标签维度上做数据统计，用统计结果作为量化结果。



##### 3. 黑盒子

**采用机器学习方法**，**学习出人类无法直观理解的稠密向量**，在推荐系统中承担的作用非常大。比如：

- 使用**浅语义模型**构建用户阅读兴趣；
- 采用**矩阵分解**得到的隐因子；
- 采用**深度模型学习用户的 Embedding 向量**；

这个方法的缺点就是得到的用户画像数据通常是不可解释的，不能直接被人看懂。



------

### 从文本到用户画像

文本数据是互联网产品中最常见的信息表达形式，数量多、处理快、存储小，常见的文本数据可以有：

- 对用户来说，包括注册时候的姓名、性别、爱好，发表的评论等；
- 对于物品，比如物品的标题、描述、物品本身的内容（一般是新闻资讯类）、其他基本属性的文本等；

接下来会介绍一些从文本数据建立用户画像的方法。

#### 构建用户画像

根据用户和物品的文本信息构建出一个基础版本的用户画像，通常步骤是这样的：

1. **结构化文本**：把所有非结构化的文本结构化，去粗取精，保留关键信息；
2. **标签选择：** 根据用户行为数据，合并用户和物品的结构化信息

第一步是最关键也是最基础，其准确性、粒度、覆盖面都决定了用户画像的质量。这一步主要用到文本挖掘算法，接下来会介绍常用的文本挖掘算法；

第二步则是根据用户历史行为把物品画像传递给用户。

下面会分别介绍这两步中的一些做法和常用算法。

#### 结构化文本

一般原始的文本数据常常是自然语言描述的，也就是“非结构化”的，但计算机处理数据，只能采用结构化的数据索引，检索，然后向量化再计算，因此对于文本数据需要先进行结构化，再进行后续的处理。

对于文本信息，可以利用成熟的 NLP  算法分析得到的信息有以下几种：

1. **关键词提取**：最基础的标签来源，也为其他文本分析提供基础数据，**常用的算法是 TF-IDF 和 TextRank**；
2. **实体识别**：识别一些名词，包括人物、位置和地点、著作、影视剧、历史事件和热点事件等，**常用基于词典的方法结合 CRF 模型**；
3. **内容分类**：将文本按照分类体系分类，用分类来表达较粗粒度的结构化信息；
4. **文本**：在无人制定分类体系的前提下，通过**无监督算法将文本划分成多个类簇**也是很常见的，**类簇编号也是用户画像的常见构成**；
5. **主题模型**：从大量已有文本中学习主题向量，然后再预测新的文本在各个主体上的概率分布情况，这也是一种聚类思想，主题向量也不是标签形式，也是用户画像的常用构成；
6. **嵌入**：即 Embedding，从词到篇章，都可以学习这种嵌入表达，**它的目标是挖掘出字面意思之下的语义信息，并用有限的维度表达出来**。

##### 1. TF-IDF

TF 全称是 Term Frequency，即词频，而 IDF 是 Inverse Document Frequency, 是逆文档频率。这个方法提取关键词的思想很朴素：

> 在一篇文章中反复出现的词会很重要，在所有文本中都出现的词更不重要。

根据这思想分别量化成 TF 和 IDF 两个指标：

- TF：词频，在要提取的文本中出现的次数；
- IDF：在所有文本中，统计每个词出现在多少文本中，记为 n，也就是文档频率，而文本的数量记为 N。

所以 IDF 的计算公式如下：
$$
IDF = log\frac{N}{n+1}
$$
它有这么几个特点：

1. 所有词的 N 都是一样的，因此**出现文本数（n）越少的词，IDF 也就越大**；
2. **分母中的 1 是防止有的词的文档频率 n 为 0**，导致得到无穷大的计算结果；
3. 对于新词，本身应该是 n=0，**但也可以默认赋值为所有词的平均文档频率**。

TF-IDF 的最终计算公式就是 TF * IDF ，这样可以计算每个词语的一个权重，根据权重来筛选关键词的方式通常有这两种：

1. **选择 top-k 的词语**，简单直接，缺点是需要考虑 k 的取值，如果能提取的词语少于 k 个，那所有词都是关键词，这就是不合理的；
2. **计算所有词权重的平均值，取权重大于平均值的词语作为关键词**。

另外，根据实际场景，可能会加入一些过滤条件，比如只提取动词和名词作为关键词。

##### 2. TextRank

**TextRank** 是著名的 PageRank 算法的衍生算法之一，PageRank 算法是谷歌用来衡量网页重要性的算法，因此 TextRank 算法的思想也比较相似，可以概括为：

1. 首先在文本中，设定一个窗口宽度，比如 k 个词，**统计窗口内的词之间的共现关系**，**将其看成无向图**（图就是网络，由存在连接关系的节点构成，所谓无向图，就是节点之间的连接关系不考虑从谁出发，有关系即可）；
2. 所有词的初始化重要性都是 1；
3. 每个节点把自己的权重平均分配给“和自己有连接”的其他节点；
4. 每个节点将所有其他节点分给自己的权重之和，作为自己的新权重；
5. 如此反复迭代第3、4两步，直到所有的节点权重收敛为止。

通过这个算法计算得到的词语权重，会呈现这样的特点：**有共现关系的会互相支持对方成为关键词**。

##### 3. 内容分类

在门户网站的时代，每个网站都有自己的频道体系，这个频道体系就是一个非常大的内容分类体系，而到了现在的移动互联网时代，新闻资讯类的 app 也会对不同的新闻进行分类到对应的不同频道下，比如热门、娱乐、体育、科技、金融等这样的分类体系，**这种做法可以得到最粗粒度的结构化信息，也是在用户冷启动时探索用户兴趣的方法**。

长文本的内容分类可以提取很多信息，但短文本的内容分类则因为可提取信息较少而比较困难。短文本分类方面经典的算法是 **SVM**，在工具上现在最常用的是 Facebook 开源的 **FastText**。

##### 4. 实体识别

**命名实体识别**（Named-Entity Recognition, NER）在 NLP 中常常被认为是**序列标注问题**，和分词、词性标注属于同一类问题。

所谓序列标注问题，就是给定一个字符序列，从左到右遍历每个字符，一边遍历一边对每个字符分类，分类的体系因序列标注问题不同而不同：

1. **分词问题**：划分词语，对每个字符分类是 “词开始”，“词中间”，“词结束” 三类之一；
2. **词性标注**：对每个分好的词，**分类为定义的词性集合之一**；
3. **实体识别**：对每个分好的词，**识别为定义的命名实体集合之一**

常用的算法就是**隐马尔可夫模型(HMM)**或者条件随机场(CRF)。

另外还有比较实用的非模型做法：**词典法**。提前准备好各种实体的词典，使用 `trie-tree`  数据结构存储，拿着分好的词去词典里找，找到个某个词就认为是提前定义好的实体了。

在工业级别的工具上，spaCy 比 NLTK 在效率上优秀一些。

##### 5. 聚类

目前常用的聚类方法主要是**主题模型**，同样作为无监督算法，**以 LDA 为代表的主题模型**能够更准确地抓住主题，**并且能够得到软聚类的效果**，**即每个文本可以属于多个类簇**。

LDA 模型需要设定主题个数，如果有时间，**可以对主题个数** K 做一些实验进行挑选，方法是**每次计算 K 个主题两两之间的平均相似度，选择一个较低的 K 值**；但如果时间不足，那么在推荐系统领域，只要计算资源够用，**主题数量可以尽量多一些**；

另外，需要注意的是，得到文本在各个主题上的分布，可以保留概率最大的前几个主题作为文本的主题。

LDA **工程上的困难在于并行化**，如果文本数量没有到海量程度，提高单机配置是可以的，开源的训练工具有 Gensim，PLDA 等。



##### 6. 词嵌入

词嵌入，即 Word Embedding，前面的几种方案，除了 LDA，其他都是得到一些标签，而且都是稀疏的，而词嵌入可以为每个词学习得到一个稠密的向量。

简单说，一个词语可能隐藏很多语义信息，比如北京，可能包含“首都、中国、北方、直辖市、大城市”等等，**这些语义在所有文本上是有限的**，比如 128 个，所以可以用一个 128 维的向量表达每个词语，**向量中各个维度上的值大小代表了词包含各个语义的多少**。

这些向量的用途有：

1. 计算词语之间的相似度，扩充结构化标签；
2. 累加得到一个文本的稠密向量；
3. 用于聚类

在这方面最著名的算法就是 **Word2Vec**，它是用浅层神经网络学习每个词语的向量表达，其最大的贡献是在工程技巧上的优化，使得百万词的规模在单机上可以几分钟就跑出结果。

#### 标签选择

完成第一步的结构化文本信息后，可以得到标签（关键词、分类等）、主题、词嵌入向量，接下来就是第二步，如何将物品的结构化信息给用户呢？

第一种做法是非常简单粗暴，直接将用户产生过行为的物品标签累积在一起；

第二种方法则是这样一个思路：将用户对物品的行为，是否有消费看成一个分类问题，用户通过实际行动帮助我们标注了若干数据，**那么挑选出他实际感兴趣的特性就变成了特征选择问题**。

最常用的是两个方法：**卡方检验(CHI)和信息增益(IG)**。基本思想是：

1. 把物品的结构化内容看成文档；
2. 把用户对物品的行为看成是类别；
3. 每个用户看见过的物品就是一个文本集合；
4. 在这个文本集合上采用特征选择算法选出每个用户关心的东西。

##### 1. 卡方检验

卡方检验是一种监督学习算法，即需要提供分类标注信息，为什么需要呢？因为在文本分类任务重，挑选管检测就是为了分类任务服务；

**卡方检验本质上是在检验“词和某个类别 C 相互独立“这个假设是否成立**，如果偏离越大，说明这个词很可能属于类别 C，这个词就是关键词了。

具体来说，计算一个词 Wi 和 一个类别 Cj 的卡方值，需要统计四个值：

1. 类别为 Cj 的文本中出现词语 Wi 的文本数 A；
2. 词 Wi 在非 Cj 的文本中出现的文本数 B；
3. 类别为 Cj 的文本中没有出现词语 Wi 的文本数 C；
4. 词 Wi 在非 Cj 的文本中没有出现的文本数 D。

用表格在展示如下：

|  卡方检验   | 属于类别Cj | 不属于类别Cj |   总计    |
| :---------: | :--------: | :----------: | :-------: |
|  包含词 Wi  |     A      |      B       |    A+B    |
| 不包含词 Wi |     C      |      D       |    C+D    |
|    总计     |    A+C     |     B+D      | N=A+B+C+D |

卡方值的计算公式如下：
$$
x^2(Wi,Cj) = \frac{N(AD-BC)^2}{(A+C)(A+B))(B+D)(C+D)}
$$
对于这个计算，有这几点说明：

1. 每个词和每个类别都要计算，只要对其中一个类别有帮助的词都应该留下；
2. 因为是比较卡方值的大小，可以不需要 N ，因为它是总的文本数，每个词都一样；
3. 卡方值越大，表示离“词语和类别相互独立”的假设越远，也就是词语和类别更可能是互相不独立，也就是词语可能就是关键词。



##### 2. 信息增益

**信息增益也是一种有监督的关键词选择方法**，需要标注信息。

首先要了解**信息熵**这个概念，它其实指的是对一件事情的不确定性的大小，比如给定任意一个文本，让你猜它属于什么类别，如果每个类别的文本数量差不多，那么肯定不好猜，但是假如少数类别的文本，比如类别C的文本占据了90%的数量，那么可以猜它是类别 C 的猜中概率就很高。

这两种情况的区别激素信息熵不同：

1. 各个类别的文本数量差不多时，信息熵比较大；
2. 少数类别的文本数量明显较多时，信息熵就较小。

接下来，假如从一堆文本中挑出包含有词语 W 的文本数，再来猜任意一条文本的类别时，还是会存在上述两种情况，但如果在整个文本上的情况是 1，挑出包含词 W 后的情况是 2，那么这种情况就说明 W 发挥了很大作用，这个过程其实也就是信息增益了。

信息增益计算的步骤如下：

1. 统计全局文本的信息熵；
2. 统计每个词的条件熵，也就是知道一个词后再统计文本的信息熵，这里需要统计包含和不包含词两部分的信息熵，再按照各自文本比例加权平均；
3. 两者相减就是每个词的信息增益。

**信息增益应用最广的就是决策树分类算法**，经典的决策树分类算法挑选分裂点时就是计算每个属性的信息增益，始终选择信息增益最大的节点作为分裂节点。

卡方检验和信息增益的不同之处：前者是针对每个行为单独筛选一套标签，后者是全局统一筛选。

------

### 小结

这篇文章先是介绍了什么是用户画像，常用的构建用户画像的例子，然后介绍了从文本数据来构建用户画像的方法，以及如何结合物品信息和用户信息。

------

参考：

- 《推荐系统三十六式》第4-5节课程

