
> 2019 年第 篇，总 篇文章




---
### 5. 支持向量机（SVM)

#### 简述

**定义**：SVM 是一种二类分类模型，其基本模型定义为**特征空间上的间隔最大的线性分类器**，即支持向量机的学习策略便是**间隔最大化**，最终可转化为一个凸二次规划问题的求解。

或者简单的可以理解为就是在高维空间中寻找一个合理的超平面将数据点分隔开来，其中涉及到非线性数据到高维的映射以达到数据线性可分的目的。

训练数据线性可分时，通过**硬间隔最大化**，学习一个线性分类器，即**线性可分支持向量机**，又称为硬间隔支持向量机；训练数据近似线性可分时，通过**软间隔最大化**，也学习一个线性分类器，即线性支持向量机，也称为软间隔支持向量机；训练数据线性不可分时，通过使用**核技巧和软间隔最大化**，学习非线性支持向量机。

原始的 SVM 是一个二类分类器，但后续针对多类分类问题，也进行了拓展，有以下几种改进来实现多类分类问题：

1.直接法

直接在目标函数上进行修改，将**多个分类面的参数求解合并到一个最优化问题**中，通过求解该优化就可以实现多分类。

但是计算复杂度很高，实现起来较为困难。一般很少使用这种方法

2.间接法，间接法又分为以下几种：

- **一对多**：每次训练的时候设置其中某个类为一类，其余所有类为另一个类。

比如有 A,B,C,D 四个类，第一次 A 是一个类，B,C,D 是一个类，训练一个分类器，第二次 B 是一个类，然后 A,C,D 是一个类，训练一个分类器，依次类推。因此，如果总共有 n 个类，最终将训练 n 个分类器。

测试的时候，将测试样本都分别送入所有分类器中，取得到最大值的类别作为其分类结果。这是因为到分类面距离越大，分类越可信。

这种方法的**优点是每个优化问题的规模比较小，而且分类速度很快**，因为分类器数目和类别数目相同；但是，有时会出现这样两种情况：对一个测试样本，每个分类器都得到它属于分类器所在类别；或者都不属于任意一个分类器的类别。**前者称为分类重叠现象，后者叫不可分类现象**。前者可以任意选择一个结果或者就按照其到每个超平面的距离来分，哪个远选哪个类别；而后者只能分给新的第 n+1 个类别了。**最大的缺点还是由于将 n-1 个类别作为一个类别，其数目会数倍于只有 1 个类的类别，这样会人为造成数据集偏斜的问题**。

- **一对一**：任意两个类都训练一个分类器，预测的时候通过投票选择最终结果。

这个方法**同样会有分类重叠的现象，但不会有不可分类现象**，因为不可能所有类别的票数都是 0。这种方法会比较高效，每次训练使用的样本其实就只有两类数据，而且预测会比较稳定，但是**缺点是预测时间会很久**。


#### 优缺点

##### 优点

1. 使用核函数可以向高维空间进行映射
2. 使用核函数可以解决非线性的分类
3. 分类思想很简单，就是将样本与决策面的间隔最大化
4. 分类效果较好

##### 缺点

1. 对大规模数据训练比较困难
2. 无法直接支持多分类，但是可以使用间接的方法来做
3. 噪声也会影响SVM的性能，因为SVM主要是由少量的支持向量决定的。

#### 代码实现

线性 SVM 的代码实现：

```python
from sklearn import datasets
import numpy as np
from sklearn.cross_validation import train_test_split

iris = datasets.load_iris() # 由于Iris是很有名的数据集，scikit-learn已经原生自带了。
X = iris.data[:, [2, 3]]
y = iris.target # 标签已经转换成0，1，2了
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0) # 为了看模型在没有见过数据集上的表现，随机拿出数据集中30%的部分做测试

# 为了追求机器学习和最优化算法的最佳性能，我们将特征缩放
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
sc.fit(X_train) # 估算每个特征的平均值和标准差
sc.mean_ # 查看特征的平均值，由于Iris我们只用了两个特征，所以结果是array([ 3.82857143,  1.22666667])
sc.scale_ # 查看特征的标准差，这个结果是array([ 1.79595918,  0.77769705])
X_train_std = sc.transform(X_train)
# 注意：这里我们要用同样的参数来标准化测试集，使得测试集和训练集之间有可比性
X_test_std = sc.transform(X_test)
X_combined_std = np.vstack((X_train_std, X_test_std))
y_combined = np.hstack((y_train, y_test))

# 导入SVC
from sklearn.svm import SVC
svm = SVC(kernel='linear', C=1.0, random_state=0) # 用线性核，你也可以通过kernel参数指定其它的核。
svm.fit(X_train_std, y_train)
# 打印决策边界，这个函数是我自己写的，如果你想要的话，我发给你
plot_decision_regions(X_combined_std, y_combined, classifier=svm, test_idx=range(105,150))
plt.xlabel('petal length [standardized]')
plt.ylabel('petal width [standardized]')
plt.legend(loc='upper left')
plt.show()
```

接下来是使用非线性 SVM 的代码：

```python
svm = SVC(kernel='rbf', random_state=0, gamma=x, C=1.0) # 令gamma参数中的x分别等于0.2和100.0
svm.fit(X_train_std, y_train) # 这两个参数和上面代码中的训练集一样
plot_decision_regions(X_combined_std, y_combined, classifier=svm, test_idx=range(105,150))
plt.xlabel('petal length [standardized]')
plt.ylabel('petal width [standardized]')
plt.legend(loc='upper left')
plt.show()
```

### 6. 朴素贝叶斯

#### 简述

> **朴素贝叶斯**是基于贝叶斯定理与特征条件独立假设的分类方法。

**贝叶斯定理**是基于条件概率来计算的，条件概率是在已知事件 B 发生的前提下，求解事件 A 发生的概率，即
$$
P(A|B)=\frac{P(AB)}{P(B)}
$$

所以贝叶斯定理如下所示：

$$
P(B|A) = \frac{P(A|B)P(B)}{P(A)}
$$


朴素贝叶斯分类器可表示为：

$$
f(x)=argmax P(y_{k})\prod_{i=1}^{n}P(x_{i}|y_{k})
$$

这里 P(y_k) 是先验概率，而 P(y_k|x) 则是后验概率，朴素贝叶斯的目标就是最大化后验概率，这等价于期望风险最小化。

朴素贝叶斯根据特征是否离散，分为三种模型，如下所示：

- **贝叶斯估计/多项式模型**：当特征是离散的时候，使用该模型；
- **高斯模型**：特征是连续的时候采用该模型；
- **伯努利模型**：特征是离散的，且取值只能是 0 和 1。


#### 优缺点

##### 优点

- 对小规模的数据表现很好，适合多分类任务，适合增量式训练。

##### 缺点

- 对输入数据的表达形式很敏感（离散、连续，值极大极小之类的）。

#### 对比逻辑回归和朴素贝叶斯

##### 相同点

1. 两者都是**对特征的线性表达**；
2. 两者建模的都是条件概率，对最终求得的分类结果有**很好的解释性**。

##### 与逻辑回归的不同

1. **朴素贝叶斯是一个生成模型**，在计算 P(y|x) 之前，先要从训练数据中计算 P(x|y) 和 P(y) 的概率，从而利用贝叶斯公式计算 P(y|x)。

   **逻辑回归是一个判别模型**，它通过在训练数据集上最大化判别函数 P(y|x) 学习得到，不需要知道 P(x|y) 和 P(y) 。

2. 朴素贝叶斯是建立在**条件独立假设**基础之上的，设特征 X 含有n个特征属性（X1，X2，...Xn），那么在给定Y的情况下，X1，X2，...Xn是条件独立的。

   逻辑回归的限制则要**宽松很多**，如果数据满足条件独立假设，能够取得非常好的效果；当数据不满足条件独立假设时，逻辑回归仍然能够通过调整参数让模型最大化的符合数据的分布，从而训练得到在现有数据集下的一个最优模型。

3. **当数据集比较小的时候，应该选用Naive Bayes**，为了能够取得很好的效果，数据的需求量为 O(log n)

   **当数据集比较大的时候，应该选用Logistic Regression**，为了能够取得很好的效果，数据的需求量为 O( n)



#### 代码实现

下面是使用`sklearn`的代码例子，分别实现上述三种模型,例子来自 [朴素贝叶斯的三个常用模型：高斯、多项式、伯努利](http://www.letiantian.me/2014-10-12-three-models-of-naive-nayes/)。

首先是高斯模型的实现：

```python
>>> from sklearn import datasets
>>> iris = datasets.load_iris()
>>> iris.feature_names  # 四个特征的名字
['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']
>>> iris.data
array([[ 5.1,  3.5,  1.4,  0.2],
       [ 4.9,  3. ,  1.4,  0.2],
       [ 4.7,  3.2,  1.3,  0.2],
       [ 4.6,  3.1,  1.5,  0.2],
       [ 5. ,  3.6,  1.4,  0.2],
       [ 5.4,  3.9,  1.7,  0.4],
       [ 4.6,  3.4,  1.4,  0.3],
       [ 5. ,  3.4,  1.5,  0.2],
       ......
       [ 6.5,  3. ,  5.2,  2. ],
       [ 6.2,  3.4,  5.4,  2.3],
       [ 5.9,  3. ,  5.1,  1.8]]) #类型是numpy.array
>>> iris.data.size  
600  #共600/4=150个样本
>>> iris.target_names
array(['setosa', 'versicolor', 'virginica'], 
      dtype='|S10')
>>> iris.target
array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,....., 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ......, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
>>> iris.target.size
150
>>> from sklearn.naive_bayes import GaussianNB
>>> clf = GaussianNB()
>>> clf.fit(iris.data, iris.target)
>>> clf.predict(iris.data[0])
array([0])   # 预测正确
>>> clf.predict(iris.data[149])
array([2])   # 预测正确
>>> data = numpy.array([6,4,6,2])
>>> clf.predict(data)
array([2])  # 预测结果很合理
```

接着，多项式模型如下：

```python
>>> import numpy as np
>>> X = np.random.randint(5, size=(6, 100))
>>> y = np.array([1, 2, 3, 4, 5, 6])
>>> from sklearn.naive_bayes import MultinomialNB
>>> clf = MultinomialNB()
>>> clf.fit(X, y)
MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)
>>> print(clf.predict(X[2]))
[3]
```

值得注意的是，多项式模型在训练一个数据集结束后可以继续训练其他数据集而无需将两个数据集放在一起进行训练。在 sklearn 中，MultinomialNB() 类的partial_fit() 方法可以进行这种训练。这种方式特别适合于训练集大到内存无法一次性放入的情况。

在第一次调用 `partial_fit()`  时需要给出所有的分类标号。

```python
>>> import numpy
>>> from sklearn.naive_bayes import MultinomialNB
>>> clf = MultinomialNB() 
>>> clf.partial_fit(numpy.array([1,1]), numpy.array(['aa']), ['aa','bb'])
GaussianNB()
>>> clf.partial_fit(numpy.array([6,1]), numpy.array(['bb']))
GaussianNB()
>>> clf.predict(numpy.array([9,1]))
array(['bb'], 
      dtype='|S2')
```

伯努利模型如下：

```python
>>> import numpy as np
>>> X = np.random.randint(2, size=(6, 100))
>>> Y = np.array([1, 2, 3, 4, 4, 5])
>>> from sklearn.naive_bayes import BernoulliNB
>>> clf = BernoulliNB()
>>> clf.fit(X, Y)
BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)
>>> print(clf.predict(X[2]))
[3]
```

### 7. KNN 算法

#### 简述

> k 近邻（KNN)是一种基本分类与回归方法。

其思路如下：给一个训练数据集和一个新的实例，在训练数据集中找出与这个新实例最近的$k$个训练实例，然后统计最近的$k$个训练实例中**所属类别计数最多的那个类**，就是新实例的类。其流程如下所示：

1. 计算训练样本和测试样本中每个样本点的距离（常见的距离度量有欧式距离，马氏距离等）；
2. 对上面所有的距离值进行排序；
3. 选前 k 个最小距离的样本；
4. 根据这 k 个样本的标签进行投票，得到最后的分类类别；

KNN 的特殊情况是 k=1 的情况，称为**最近邻算法**。对输入的实例点（特征向量）x，最近邻法将训练数据集中与 x 最近邻点的类作为其类别。

#### 三要素

1. k 值的选择
2. 距离的度量（常见的距离度量有欧式距离，马氏距离）
3. 分类决策规则（多数表决规则）

#### k 值的选择

1. **k 值越小表明模型越复杂，更加容易过拟合**，其**偏差小，而方差大**
2. 但是 **k 值越大，模型越简单**，如果 k=N 的时候就表明无论什么点都是**训练集中类别最多**的那个类，这种情况，则是**偏差大，方差小**。

> 所以一般 k 会取一个**较小的值，然后用过交叉验证来确定**
> 这里所谓的交叉验证就是将样本划分一部分出来为预测样本，比如 95% 训练，5% 预测，然后 k 分别取1，2，3，4，5 之类的，进行预测，计算最后的分类误差，选择误差最小的 k

#### 距离的度量

KNN 算法使用的距离一般是欧式距离，也可以是更一般的 $L_p$ 距离或者马氏距离，其中 $L_p$ 距离定义如下：

$$
L_p(x_i, x_j) = (\sum_{l=1}^n |x_i^{(l)} - x_j^{(l)} |^p)^{\frac{1}{p}}
$$


#### KNN的回归

在找到最近的 k 个实例之后，可以计算这 k 个实例的平均值作为预测值。或者还可以给这 k 个实例添加一个权重再求平均值，**这个权重与度量距离成反比（越近权重越大）**。

#### 优缺点

优点

1. **思想简单，理论成熟，既可以用来做分类也可以用来做回归**；
2. 可用于**非线性分类**；
3. 训练时间复杂度为$O(n)$；
4. 准确度高，对数据没有假设，对**异常值**不敏感；

#### 缺点

1. **计算量大**；
2. **样本不平衡问题**（即有些类别的样本数量很多，而其它样本的数量很少）；
3. 需要**大量的内存**；

#### 代码实现

使用`sklearn`的简单代码例子：

```python
#Import Library
from sklearn.neighbors import KNeighborsClassifier

#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset
# Create KNeighbors classifier object model 

KNeighborsClassifier(n_neighbors=6) # default value for n_neighbors is 5

# Train the model using the training sets and check score
model.fit(X, y)

#Predict Output
predicted= model.predict(x_test)
```

最后，**在用KNN前你需要考虑到：**

- KNN的计算成本很高
- 所有特征应该**标准化数量级**，否则数量级大的特征在计算距离上会有偏移。
- 在进行KNN前**预处理数据**，例如去除异常值，噪音等。




---
### 小结



---
#### 参考

- 《统计学习方法》
- [SVM详解(包含它的参数C为什么影响着分类器行为)-scikit-learn拟合线性和非线性的SVM](http://blog.csdn.net/xlinsist/article/details/51311755)
- [机器学习常见算法个人总结（面试用）](http://kubicode.me/2015/08/16/Machine%20Learning/Algorithm-Summary-for-Interview/)
- [SVM-支持向量机算法概述](http://blog.csdn.net/passball/article/details/7661887)
- [机器学习算法与Python实践之（二）支持向量机（SVM）初级](http://blog.csdn.net/zouxy09/article/details/17291543)
- [机器学习算法与Python实践之（三）支持向量机（SVM）进阶](http://blog.csdn.net/zouxy09/article/details/17291805)
- [机器学习算法与Python实践之（四）支持向量机（SVM）实现](http://blog.csdn.net/zouxy09/article/details/17292011)
- [【模式识别】SVM核函数](http://blog.csdn.net/xiaowei_cqu/article/details/35993729)
- [SVM的核函数如何选取?--知乎](https://www.zhihu.com/question/21883548)
- [朴素贝叶斯理论推导与三种常见模型](http://blog.csdn.net/u012162613/article/details/48323777)
- [朴素贝叶斯的三个常用模型：高斯、多项式、伯努利](http://www.letiantian.me/2014-10-12-three-models-of-naive-nayes/)